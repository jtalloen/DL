{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Zo-OKnwqq4U"
   },
   "source": [
    "# Spectrograms NN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tc51H5FTqq4W"
   },
   "source": [
    "First import the necessary packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBh0POewqq4X"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "cuda\n",
    "\n",
    "\n",
    "# data = np.load('dev.npy', allow_pickle = True)\n",
    "# datacat = np.concatenate(data, axis = 0)\n",
    "# print(datacat[slice(100,102)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AR4DcZjXqq4c"
   },
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lity8twlqq4d"
   },
   "source": [
    "Will be getting two dataset, a train and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "8OxdaxCKqq4e",
    "outputId": "836e2efe-5271-45cf-84c0-2d75ef13e39c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1100,)\n",
      "(675836, 40) (675836, 1)\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        print(np.load(self.X, allow_pickle = True).shape)\n",
    "        self.Xcat = np.concatenate(np.load(self.X, allow_pickle = True), axis = 0)\n",
    "        self.Ycat = np.concatenate(np.load(self.Y, allow_pickle = True), axis = 0)\n",
    "\n",
    "        self.k = 0\n",
    "        if self.k > 0:\n",
    "          zeros = np.zeros((self.k, 40)).astype(float)\n",
    "          self.Xcat = np.append(zeros, self.Xcat, axis = 0)  \n",
    "          self.Xcat = np.append(self.Xcat, zeros, axis = 0)       \n",
    "\n",
    "        self.x = self.Xcat\n",
    "        self.y = np.reshape(self.Ycat, (len(self.Ycat), 1))\n",
    "        print(self.x.shape, self.y.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        # turn Y to long to pass into cross entropy?\n",
    "\n",
    "        X = self.x[slice(index-self.k, index+self.k+1)].reshape(-1).astype(float)\n",
    "        Y = self.y[index].astype(int)\n",
    "        return X, Y\n",
    "\n",
    "    # dataset loader, turn data into 2D array, then slice.  \n",
    "\n",
    "num_workers = 4 if cuda else 0 \n",
    "    \n",
    "# Training\n",
    "train_dataset = MyDataset('dev.npy', 'dev_labels.npy')\n",
    "\n",
    "train_params = dict(shuffle=True, batch_size=64, num_workers=num_workers, pin_memory=False) if cuda\\\n",
    "                    else dict(shuffle=True, batch_size=64)\n",
    "\n",
    "train_load = data.DataLoader(train_dataset, **train_params)\n",
    "\n",
    "# # Testing\n",
    "# test_dataset = MyDataset('dev.npy', 'dev_labels.npy')\n",
    "\n",
    "# test_params = dict(shuffle=False, batch_size=256, num_workers=num_workers, pin_memory=True) if cuda\\\n",
    "#                     else dict(shuffle=False, batch_size=1)\n",
    "\n",
    "# test_load = data.DataLoader(test_dataset, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "NkcsTnHNMVj9",
    "outputId": "c3a96649-64a3-4a2b-8818-ab48b750d40e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> tensor([-276.2644, -165.0078, -204.2537, -166.4714, -206.1901, -235.2212,\n",
      "        -189.5758, -218.4456, -168.3412, -212.7154, -175.5260, -258.9223,\n",
      "        -153.3019, -195.4387, -239.7365, -232.7189, -182.9045, -263.9122,\n",
      "        -217.3967, -275.5172, -168.0723, -238.3570, -220.0867, -232.6366,\n",
      "        -247.5462, -226.2658, -188.4547, -262.8099, -202.5450, -200.9244,\n",
      "        -261.6183, -122.4430, -209.3424, -190.1110, -199.0182, -215.4001,\n",
      "        -236.8663, -221.0992, -179.4614, -217.6890], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for (x, y) in train_load:\n",
    "    print(type(x[0]),x[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yEEEK--gqq4g"
   },
   "source": [
    "### Define a NN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pc1WRJfqq4h"
   },
   "outputs": [],
   "source": [
    "class simpleMLP(nn.Module):\n",
    "    def __init__(self, linear_layers, num_batch_norm):\n",
    "        super(simpleMLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        self.linear_layers = linear_layers\n",
    "        self.num_batch_norm = num_batch_norm\n",
    "        self.bn = num_batch_norm > 0\n",
    "\n",
    "        # create the layers\n",
    "        for i in range(len(linear_layers) - 2):\n",
    "            layers.append(nn.Linear(linear_layers[i], linear_layers[i+1]))\n",
    "            if self.bn and i < num_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(linear_layers[i+1]))\n",
    "            layers.append(nn.Sigmoid())\n",
    "        layers.append(nn.Linear(linear_layers[-2], linear_layers[-1]))\n",
    "\n",
    "        # combine final net and assign\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12C6ZE68qq4k"
   },
   "source": [
    "### Create the model and define our loss criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FVLglKRDqq4l"
   },
   "source": [
    "May want to add optimal learning rate schedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 824
    },
    "colab_type": "code",
    "id": "C2IPQzSQqq4l",
    "outputId": "1587a6be-c1ca-4958-9a85-0cf9f79a2536"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simpleMLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=40, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Sigmoid()\n",
      "    (3): Linear(in_features=256, out_features=138, bias=True)\n",
      "  )\n",
      ") CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "model = simpleMLP([40, 256, 138], 1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)\n",
    "print(model, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fQs5H3V3qq4o"
   },
   "source": [
    "### Our training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vhh2JpM-qq4p"
   },
   "source": [
    "Basically we do a foward pass, then compute the loss, backward pass, take a step and reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fLejyyvfqq4q"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_load, criterion, optimizer):\n",
    "    # indicate that we are training \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # start timer and start iterating\n",
    "    start_train = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_load):   \n",
    "        data = data.to(device)\n",
    "        target = target.to(device) # all data & model on same device\n",
    "        \n",
    "        # forward, then backward, then step\n",
    "        outputs = model(data.float())\n",
    "        print(outputs)\n",
    "        loss = criterion(torch.LongTensor(outputs), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
    "\n",
    "        # accumulate loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # end timer and take average loss\n",
    "    end_train = time.time()\n",
    "    running_loss /= len(train_load)\n",
    "    \n",
    "    return running_loss, end_train, start_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MWo4JyF9qq4t"
   },
   "source": [
    "### Writing our test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tGOF0uQEqq4t"
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_load, criterion, testout):\n",
    "    with torch.no_grad():\n",
    "        # indicate that we are evaluating model\n",
    "        model.eval()\n",
    "        \n",
    "        # initialize errors to 0\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0    \n",
    "        \n",
    "        # start iterating\n",
    "        for batch_idx, (data, target) in enumerate(test_load):   \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # run forward pass and then compute loss\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target).detach()\n",
    "            \n",
    "            # get predictions \n",
    "            _, predicted = torch.max(outputs.data, 0)\n",
    "            \n",
    "            # write predictions and calculate correct predictions / loss\n",
    "            testout.write(str(predicted), '\\n')\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # calculate average loss and accuract\n",
    "        running_loss /= len(test_load)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "\n",
    "        return running_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LjhRBlYSqq4w"
   },
   "source": [
    "### Initialize and run our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "colab_type": "code",
    "id": "MeqvX0EHqq4x",
    "outputId": "a12f0384-97ac-45a1-ba8a-45430691e927"
   },
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "\n",
    "Train_loss = []\n",
    "Test_loss = []\n",
    "Test_acc = []\n",
    "\n",
    "testout = open(\"submission.csv\", \"w\")\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    \n",
    "    # Training and outputting loss\n",
    "    train_loss, end_train, start_train = train_epoch(model, train_load, criterion, optimizer)\n",
    "    print('Training Loss: ', train_loss, 'Time: ',end_train - start_train, 's')\n",
    "    \n",
    "     # Test and ouputting\n",
    "     test_loss, test_acc = test_model(model, test_load, criterion, testout)\n",
    "     print('Testing Loss: ', test_loss)\n",
    "     print('Testing Accuracy: ', test_acc, '%')    \n",
    "    \n",
    "     Train_loss.append(train_loss)\n",
    "     Test_loss.append(test_loss)\n",
    "     Test_acc.append(test_acc)\n",
    "\n",
    "     print('='*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW1P2 Spectrograms Kaggle.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
