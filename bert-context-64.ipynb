{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import *\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print(\"cuda: \", cuda)\n",
    "# num_workers = 8 if cuda else 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dataset and DataLoader</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_dataset=pickle.load(open(\"../nutella/datasets/predict-importance-an-sorted.pkl\", \"rb\"))\n",
    "\n",
    "train_texts=[]\n",
    "train_labels=[]\n",
    "for x in an_dataset[\"train_dataset\"]:\n",
    "    train_texts.extend([q[\"txt\"] for q in x[\"data\"]])\n",
    "    train_labels.extend([q[\"important\"] for q in x[\"data\"]])\n",
    "\n",
    "val_texts=[]\n",
    "val_labels=[]    \n",
    "for x in an_dataset[\"val_dataset\"]:\n",
    "    val_texts.extend([q[\"txt\"] for q in x[\"data\"]])\n",
    "    val_labels.extend([q[\"important\"] for q in x[\"data\"]])    \n",
    "    \n",
    "test_texts=[]\n",
    "test_labels=[]        \n",
    "for x in an_dataset[\"test_dataset\"]:\n",
    "    test_texts.extend([q[\"txt\"] for q in x[\"data\"]])\n",
    "    test_labels.extend([q[\"important\"] for q in x[\"data\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterasdf = 0\n",
    "max_len = len(train_texts[0].split())\n",
    "over_200 = 0\n",
    "over_500 = 0\n",
    "over_1000 = 0\n",
    "for s in train_texts:\n",
    "    if len(s.split()) > 32:\n",
    "        over_200 += 1\n",
    "    if len(s.split()) > 64:\n",
    "        over_500 += 1\n",
    "    if len(s.split()) > 128:\n",
    "        over_1000 += 1\n",
    "    if len(s.split()) > max_len:\n",
    "        max_len = len(s.split())\n",
    "        # print(\"=\" * 40)\n",
    "        # print(\"new max len: \" + str(max_len))\n",
    "        # print(\"=\" * 40)\n",
    "        # print(s)\n",
    "print(\"total length: \" + str(len(train_texts)))\n",
    "print(\"num over 32: \" + str(over_200))\n",
    "print(\"num over 64: \" + str(over_500))\n",
    "print(\"num over 128: \" + str(over_1000))\n",
    "print(\"max len: \" + str(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "max_seq_len = 64\n",
    "print(\"maximum sequence length for this round: \" + str(max_seq_len))\n",
    "\n",
    "for i in range(len(train_texts)):\n",
    "    if i == 0:\n",
    "        s = train_texts[i] + train_texts[i+1]\n",
    "    elif i == len(train_texts)-1:\n",
    "        s = train_texts[i-1] + train_texts[i]\n",
    "    else:\n",
    "        s = train_texts[i-1] + train_texts[i] + train_texts[i+1]\n",
    "    \n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                    s,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=max_seq_len,\n",
    "                    pad_to_max_length=True,\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "train_input_ids = torch.cat(input_ids, dim=0)\n",
    "train_attention_masks = torch.cat(attention_masks, dim=0)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for i in range(len(val_texts)):\n",
    "    if i == 0:\n",
    "        s = val_texts[i] + val_texts[i+1]\n",
    "    elif i == len(val_texts)-1:\n",
    "        s = val_texts[i-1] + val_texts[i]\n",
    "    else:\n",
    "        s = val_texts[i-1] + val_texts[i] + val_texts[i+1]\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                    s,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=max_seq_len,\n",
    "                    pad_to_max_length=True,\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "val_input_ids = torch.cat(input_ids, dim=0)\n",
    "val_attention_masks = torch.cat(attention_masks, dim=0)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "print('done loading, this takes forever')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train, Test, Metrics Utility Functions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(y_true, predicted):\n",
    "    this_results={}\n",
    "    \n",
    "    this_results[\"precision\"]=sklearn.metrics.precision_score(y_true, predicted)\n",
    "    this_results[\"recall\"]=sklearn.metrics.recall_score(y_true, predicted)    \n",
    "    this_results[\"f1\"]=sklearn.metrics.f1_score(y_true, predicted)\n",
    "    this_results[\"accuracy\"]=sklearn.metrics.accuracy_score(y_true, predicted)    \n",
    "    this_results[\"auc\"]=sklearn.metrics.roc_auc_score(y_true, predicted)    \n",
    "\n",
    "    return this_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer):\n",
    "    model.train() \n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_lens = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    print('='*40)\n",
    "    print(\"Training\", len(train_loader), \"batches\")\n",
    "    print('='*40)\n",
    "    \n",
    "    # start timer and start iterating\n",
    "    start_train = time.time()\n",
    "    for batch_idx, (data, mask, target) in enumerate(train_loader):       \n",
    "        optimizer.zero_grad()  \n",
    "        data, mask, target = data.to(device), mask.to(device), target.to(device)\n",
    "        \n",
    "        loss, logits = model(data, token_type_ids=None, attention_mask=mask, labels=target)\n",
    "        \n",
    "        # accumulate loss\n",
    "        cumulative_loss += loss.item()\n",
    "        cumulative_lens += 1   # len(target)\n",
    "        running_loss = cumulative_loss / cumulative_lens\n",
    "        \n",
    "        mid_train = time.time()\n",
    "        if batch_idx % 40 == 39:\n",
    "            print(\"Batch: \", batch_idx + 1)\n",
    "            print('Cumulative Time: {:.4f}s\\nLoss: {:.4f}'.format(mid_train - start_train, running_loss))\n",
    "            print('='*40)\n",
    "            \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step() \n",
    "        \n",
    "        # delete variables and empty cache\n",
    "        torch.cuda.empty_cache()\n",
    "        del data\n",
    "        del target\n",
    "        del mask\n",
    "    \n",
    "    # end timer and take average loss\n",
    "    end_train = time.time()\n",
    "    time_train = end_train - start_train\n",
    "    return time_train, running_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        target_total = []\n",
    "        predicted_total = []\n",
    "\n",
    "        start_test = time.time()\n",
    "        for batch_idx, (data, mask, target) in enumerate(test_loader): \n",
    "                \n",
    "            data, mask, target = data.to(device), mask.to(device), target.to(device)\n",
    "        \n",
    "            loss, logits = model(data, token_type_ids=None, attention_mask=mask, labels=target)\n",
    "\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            # print('predicted: ', predicted)\n",
    "            \n",
    "            # loss = criterion(logits, target).detach()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            mid_test = time.time()\n",
    "            if batch_idx % 40 == 39:\n",
    "                print(\"Batch: \", batch_idx + 1)\n",
    "                print('Cumulative Time: {:.4f}s\\nLoss: {:.4f}'.format(mid_test - start_test, running_loss/batch_idx))\n",
    "                print('='*40)\n",
    "            \n",
    "            target = target.data.cpu().numpy()\n",
    "            predicted = predicted.data.cpu().numpy()\n",
    "            \n",
    "            target_total.extend(target)\n",
    "            predicted_total.extend(predicted)\n",
    "            \n",
    "            # delete variables and empty cache\n",
    "            torch.cuda.empty_cache()\n",
    "            del data\n",
    "            del mask\n",
    "            del target\n",
    "        \n",
    "        results = calc_metrics(np.array(target_total), np.array(predicted_total))\n",
    "        running_loss /= len(test_loader)\n",
    "        print('Dev Loss: ', running_loss)\n",
    "        print('Results', results)\n",
    "        return running_loss, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hyperparameters and Runtime</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PARAMETERS \n",
    "\n",
    "epochs = 1\n",
    "learningRate = 2e-5\n",
    "weightDecay = 0.00004\n",
    "momentum = 0.9\n",
    "num_workers = 4\n",
    "\n",
    "print(\"train_input_ids size: \" + str(train_input_ids.size()))\n",
    "print(\"train_attention_masks size: \" + str(train_attention_masks.size()))\n",
    "print(\"train_labels size: \" + str(train_labels.size()))\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                shuffle=True,\n",
    "                batch_size = batch_size,\n",
    "                num_workers = num_workers,\n",
    "                pin_memory = True)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                shuffle=True,\n",
    "                batch_size = batch_size,\n",
    "                num_workers = num_workers,\n",
    "                pin_memory = True)\n",
    "\n",
    "print(\"length of train loader: \" + str(len(train_loader)))\n",
    "print(\"length of val loader: \" + str(len(val_loader)))\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=2, output_attentions=False, output_hidden_states=False) \n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "total_steps = len(train_loader) * epochs\n",
    "optimizer = AdamW(model.parameters(), lr=learningRate, eps=1e-8) \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"bert_context_results_sorted_an_64.txt\", \"w\")\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "    time_train, train_loss = train_epoch(model, train_loader, optimizer)\n",
    "    print(\"done training for epoch: \" + str(i))\n",
    "    val_loss, val_results = test_model(model, val_loader)\n",
    "    print(\"done calculating val loss for epoch: \" + str(i))\n",
    "\n",
    "    print('='*60)\n",
    "    print('Epoch: {:.0f}\\nTrain Time: {:.4f}s\\nTrain Loss: {:.4f}\\nVal Loss: {:.4f}'.format(i, time_train, train_loss, val_loss))\n",
    "    print('='*60)\n",
    "    f.write('\\n')\n",
    "    f.write(\"Stats for epoch \" + str(i))\n",
    "    f.write('Epoch: {:.0f}\\nTrain Time: {:.4f}s\\nTrain Loss: {:.4f}\\nVal Loss: {:.4f}'.format(i, time_train, train_loss, val_loss))\n",
    "    f.write('\\n')\n",
    "    f.write('Val Results' + str(val_results))\n",
    "    f.write('\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE/LOAD MODEL\n",
    "# path = \"./\"\n",
    "# torch.save(network.state_dict(), path)\n",
    "# network.load_state_dict(torch.load(path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
